# ðŸ“ˆ Optimizer Documentation

This file documents the **optimizers** used in the `rnn-lstm-from-scratch` project.

We focus on **implementing optimizers manually** to better understand how parameter updates work during training.

[Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network)

---

## ðŸš€ 1. Stochastic Gradient Descent (SGD)

### âœï¸ Update Rule (Math)

Given:
- Parameter **Î¸** (such as weights or biases)
- Loss function **J(Î¸)**
- Gradient **âˆ‡Î¸ J(Î¸)**

The **SGD update rule** is:

```
Î¸ := Î¸ - Î± âˆ‡Î¸ J(Î¸)
```

Where:
- **Î±** is the **learning rate**
- **âˆ‡Î¸ J(Î¸)** is the **gradient** of the loss function with respect to the parameter Î¸

---

### ðŸ“š In Context of Our RNN

If parameters are:
- **Waa**, **Wax**, **Wya**, **ba**, **by**

And gradients are:
- **dWaa**, **dWax**, **dWya**, **dba**, **dby**

Then the updates are:

```
Waa := Waa - Î± dWaa
Wax := Wax - Î± dWax
Wya := Wya - Î± dWya
ba  := ba  - Î± dba
by  := by  - Î± dby
```

---

### ðŸ§© Python Pseudocode

```python
for param_name in parameters:
    parameters[param_name] -= learning_rate * gradients["d" + param_name]
```

In our code, this is handled by the `SGDOptimizer` class.

---

### âœ… Key Properties

| Property        | Behavior                                |
|:----------------|:----------------------------------------|
| Simplicity      | Very easy to implement                  |
| Memory Usage    | Very low (no extra state needed)         |
| Convergence     | Can be slow if learning rate not tuned  |
| Instability     | Sensitive to learning rate, no momentum |

---

### ðŸ“‰ When to Use

- **Simple problems**
- **Small datasets**
- When learning rate is carefully tuned manually

---

## ðŸš€ 2. Momentum Optimizer

### âœï¸ Update Rule (Math)

Momentum adds a velocity term **v** to smooth updates:

```
v := Î² v - Î± âˆ‡Î¸ J(Î¸)
Î¸ := Î¸ + v
```

Where:
- **Î²** is the **momentum coefficient** (typically 0.9)
- **v** is the **velocity** (running sum of past gradients)
- **Î±** is the **learning rate**
- **âˆ‡Î¸ J(Î¸)** is the gradient

---

### ðŸ“š In Context of Our RNN

The parameter updates now consider *previous gradients*:

1. Update velocity:

```
v_dWaa = Î² v_dWaa - Î± dWaa
v_dWax = Î² v_dWax - Î± dWax
...
```

2. Update parameters:

```
Waa := Waa + v_dWaa
Wax := Wax + v_dWax
...
```

---

### ðŸ§© Python Pseudocode

```python
for grad, param in grads_and_vars:
    v = momentum * v - learning_rate * grad
    param += v
```

In our code, this is handled by the `MomentumOptimizer` class.

---

### âœ… Key Properties

| Property        | Behavior                                |
|:----------------|:----------------------------------------|
| Smoother updates | Less oscillation compared to SGD       |
| Faster convergence | Can speed up training significantly |
| Requires tuning | Need to choose **Î±** and **Î²** carefully |

---

### ðŸ“‰ When to Use

- Training is slow or oscillatory
- Need to escape local minima
- Common when learning simple RNNs or small LSTMs

---

## ðŸ“œ Optimizer Status

| Optimizer  | Status         |
|:-----------|:---------------|
| SGD        | âœ… Implemented |
| Momentum   | âœ… Implemented |
| RMSProp    | ðŸ”œ Planned     |
| Adam       | ðŸ”œ Planned     |

---

âœ… As we add **RMSProp** and **Adam**, we will document them here with:
- Update equations
- Python pseudocode
- Practical usage notes

---

# ðŸ› ï¸ Optimizer Code Layout

```
src/
â””â”€â”€ optimizers/
    â”œâ”€â”€ optimizer.py          # Base optimizer class
    â”œâ”€â”€ sgd_optimizer.py      # SGD optimizer
    â”œâ”€â”€ momentum_optimizer.py # Momentum optimizer
    â”œâ”€â”€ rmsprop_optimizer.py  # (planned)
    â””â”€â”€ adam_optimizer.py     # (planned)
```

---

> ðŸ’¡ **Reminder:**  
> All optimizers are implemented manually with **NumPy**, no external libraries like TensorFlow or PyTorch.

---

## ðŸ§  Learn More

- [Coursera NLP Sequence Models](https://www.coursera.org/learn/nlp-sequence-models/home/week/1)
- [Backpropagation Through Time (BPTT)](https://www.coursera.org/learn/nlp-sequence-models/lecture/bc7ED/backpropagation-through-time)
- [Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network)

---
