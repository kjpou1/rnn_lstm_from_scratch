# ðŸ“ˆ Optimizer Documentation

This file documents the **optimizers** used in the `rnn-lstm-from-scratch` project.

We focus on **implementing optimizers manually** to better understand how parameter updates work during training.

[Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network)

---

## ðŸš€ 1. Stochastic Gradient Descent (SGD)

### âœï¸ Update Rule (Math)

Given:
- Parameter **Î¸** (such as weights or biases)
- Loss function **J(Î¸)**
- Gradient **âˆ‡Î¸ J(Î¸)**

The **SGD update** is:

```
Î¸ := Î¸ - Î± * âˆ‡Î¸ J(Î¸)
```

Where:
- **Î±** is the learning rate
- **âˆ‡Î¸ J(Î¸)** is the gradient of the loss w.r.t. the parameter.

---

### ðŸ“š In Context of Our RNN

If parameters are:
- `Waa`, `Wax`, `Wya`, `ba`, `by`

And gradients are:
- `dWaa`, `dWax`, `dWya`, `dba`, `dby`

Then the updates are:

```
Waa := Waa - Î± * dWaa
Wax := Wax - Î± * dWax
Wya := Wya - Î± * dWya
ba  := ba  - Î± * dba
by  := by  - Î± * dby
```

---

### ðŸ§© Python Pseudocode

```python
for param_name in parameters:
    parameters[param_name] -= learning_rate * gradients["d" + param_name]
```

Handled by the `SGDOptimizer` class.

---

### âœ… Key Properties
| Property | Behavior |
|:---------|:---------|
| Simplicity | Very easy to implement |
| Memory Usage | Very low (no extra state needed) |
| Convergence | Can be slow if learning rate is not tuned |
| Instability | Sensitive to learning rate, no momentum |

---

### ðŸ“‰ When to Use
- **Simple problems**
- **Small datasets**
- When learning rate is carefully tuned manually

---

## ðŸš€ 2. Momentum Optimizer

### âœï¸ Update Rule (Math)

Given:
- Velocity vector **v**
- Parameters **Î¸**
- Gradients **âˆ‡Î¸ J(Î¸)**

The **Momentum** update is:

```
v := Î² * v - Î± * âˆ‡Î¸ J(Î¸)
Î¸ := Î¸ + v
```

Where:
- **Î²** is the momentum coefficient (e.g., 0.9)
- **Î±** is the learning rate

---

### ðŸ“š In Context of Our RNN

```
v_dWaa := Î² * v_dWaa - Î± * dWaa
Waa    := Waa + v_dWaa
```
(Similar updates for `Wax`, `Wya`, `ba`, and `by`.)

---

### ðŸ§  Practical Observations from Training

Choosing the right learning rate **(Î±)** and momentum **(Î²)** is critical:

| Parameter | Typical Range | Effect |
|:----------|:--------------|:-------|
| **Learning rate (Î±)** | 0.001 â€“ 0.01 | Smaller values help prevent overshooting when momentum is high |
| **Momentum coefficient (Î²)** | 0.9 â€“ 0.99 | Higher values smooth updates but can cause overshooting |

---

### âš¡ Practical Observations

- **If the learning rate is too large** (e.g., 0.1 or higher):
  - Momentum causes **overshooting** and unstable training.

- **Solution:**
  - Lower learning rate to 0.01 or 0.001.
  - Tune Î² carefully (typically start with 0.9).

- **If Î² is too high** (e.g., 0.99+):
  - Momentum builds too much and may "blow past" minima.

âœ… In scratch experiments, **MomentumOptimizer** performed **poorly with Î± = 0.1**, but improved **significantly when Î± = 0.01** or smaller.

---

### ðŸƒâ€â™‚ï¸ Quick Guide

| Setup | Behavior |
|:------|:---------|
| **High Î± + High Î²** | ðŸš€ Overshoots! Model unstable |
| **Low Î± + High Î²**  | ðŸƒâ€â™‚ï¸ Smooth fast convergence |
| **Too Low Î±**       | ðŸ¢ Very slow learning |

---

> âš¡ **Bottom line:**  
> **Momentum can accelerate convergence**, but **only if Î± and Î² are tuned together carefully**.

---

## ðŸš€ 3. RMSProp Optimizer

### âœï¸ Update Rule (Math)

Given:
- Running average of squared gradients **s**
- Parameters **Î¸**
- Gradients **âˆ‡Î¸ J(Î¸)**

The **RMSProp** update is:

```
s := Î² * s + (1 - Î²) * (âˆ‡Î¸ J(Î¸))Â²
Î¸ := Î¸ - Î± * âˆ‡Î¸ J(Î¸) / (âˆšs + Îµ)
```

Where:
- **s** is the exponentially decaying average of squared gradients
- **Î²** is the decay rate (e.g., 0.9)
- **Îµ** is a small constant to prevent division by zero (e.g., 1e-8)

---

### ðŸ“š In Context of Our RNN

```
s_dWaa := Î² * s_dWaa + (1 - Î²) * (dWaa)Â²
Waa    := Waa - Î± * dWaa / (âˆšs_dWaa + Îµ)
```
(Similar for `Wax`, `Wya`, `ba`, and `by`.)

---

### âœ… Key Properties
| Property | Behavior |
|:---------|:---------|
| Adaptive Learning Rates | Adjusts step size per parameter |
| Reduces Oscillations | Especially for noisy or sparse gradients |
| Hyperparameter Sensitivity | Requires tuning Î² and Î± |

---

### ðŸ“‰ When to Use
- **Training is noisy**
- **Gradients vary a lot**
- **Want automatic learning rate adaptation**

âœ… In practice, RMSProp speeds up convergence compared to SGD or plain momentum.

---

## ðŸš€ 4. Adam Optimizer

### âœï¸ Update Rule (Math)

Adam combines **Momentum** + **RMSProp**:

1. **First moment estimate (mâ‚œ):**

```
mâ‚œ = Î²â‚ * mâ‚œâ‚‹â‚ + (1 - Î²â‚) * âˆ‡Î¸ J(Î¸)
```

2. **Second moment estimate (vâ‚œ):**

```
vâ‚œ = Î²â‚‚ * vâ‚œâ‚‹â‚ + (1 - Î²â‚‚) * (âˆ‡Î¸ J(Î¸))Â²
```

3. **Bias correction:**

```
mÌ‚â‚œ = mâ‚œ / (1 - Î²â‚áµ—)
vÌ‚â‚œ = vâ‚œ / (1 - Î²â‚‚áµ—)
```

4. **Update parameters:**

```
Î¸ := Î¸ - Î± * mÌ‚â‚œ / (âˆšvÌ‚â‚œ + Îµ)
```

Where:
- **Î±** = learning rate
- **Î²â‚** = momentum term (first moment, e.g., 0.9)
- **Î²â‚‚** = RMSProp term (second moment, e.g., 0.999)
- **Îµ** = small value to prevent division by zero

---

### ðŸ“š In Context of Our RNN

```
Waa := Waa - Î± * mÌ‚_dWaa / (âˆšvÌ‚_dWaa + Îµ)
```
(Similar updates for `Wax`, `Wya`, `ba`, and `by`.)

---

### âœ… Key Properties
| Property | Behavior |
|:---------|:---------|
| Combines Momentum + RMSProp | âœ… |
| Adaptive Learning Rate | âœ… |
| Bias Correction | âœ… |
| Very Popular Optimizer | âœ… |

---

### ðŸ“‰ When to Use
- **Almost always** a strong default choice
- Robust to noisy gradients
- Works well without much hyperparameter tuning

âœ… In our scratch experiments, Adam delivered **fast, stable convergence**.

---

# ðŸ“œ Optimizer Status

| Optimizer | Status |
|:----------|:-------|
| SGD (vanilla) | âœ… Implemented |
| Momentum | âœ… Implemented |
| RMSProp | âœ… Implemented |
| Adam | âœ… Implemented |

---

# ðŸ› ï¸ Optimizer File Structure

```
src/
â””â”€â”€ optimizers/
    â”œâ”€â”€ optimizer.py          # Base optimizer class
    â”œâ”€â”€ sgd_optimizer.py      # SGD optimizer
    â”œâ”€â”€ momentum_optimizer.py # Momentum optimizer
    â”œâ”€â”€ rmsprop_optimizer.py  # RMSProp optimizer
    â”œâ”€â”€ adam_optimizer.py     # Adam optimizer
```

---

> ðŸ’¡ **Reminder:**  
> Our goal is to *learn by doing*, so every optimizer is implemented manually using **NumPy**, no TensorFlow or PyTorch optimizers.

---

## ðŸ§  Learn More
- [Coursera NLP Sequence Models](https://www.coursera.org/learn/nlp-sequence-models/home/week/1)
- [Backpropagation Through Time (BPTT)](https://www.coursera.org/learn/nlp-sequence-models/lecture/bc7ED/backpropagation-through-time)
- [Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network)
