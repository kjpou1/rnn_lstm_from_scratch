# ğŸ§  Activation Functions (From Scratch)

This module defines core activation functions used in training neural networks **built manually using NumPy**. Each activation follows a common interface via an abstract base class.

---

## ğŸ“¦ Structure

All activation classes inherit from:

```python
BaseActivation (abstract class)
```

Each activation must implement:

```python
@staticmethod
def forward(x: np.ndarray) -> np.ndarray:
    """Apply the activation function."""

@staticmethod
def backward(x: np.ndarray) -> np.ndarray:
    """Compute derivative of the activation w.r.t. input (âˆ‚activation/âˆ‚x)."""
```

| Method        | Purpose                                                  | Used In               |
|---------------|----------------------------------------------------------|------------------------|
| `forward(x)`  | Apply non-linearity to input                             | Forward pass (`a`)     |
| `backward(x)` | Compute local derivative of activation                   | Backward pass (`dz`)   |

---

## ğŸ§® Why Separate `forward` and `backward`?

Training a neural network involves two phases:

1. **Forward Propagation** â€“ Compute activations for each layer  
   â†’ e.g., `a = tanh(z)`
2. **Backward Propagation** â€“ Compute gradients to update weights  
   â†’ e.g., `dz = da * (1 - tanhÂ²(z))`

To support manual backpropagation, each activation defines:
- `forward()` for computing activations
- `backward()` for computing the local gradient

---

### ğŸ”„ Forwardâ€“Backward Caching (and Why We Use It)

In this project, each activation function computes its **derivative using the output from its forward pass**. That means during training:

```python
# Forward pass
a = SigmoidActivation.forward(z)

# Backward pass
dz = da * SigmoidActivation.backward(a)  # NOT passing z
```

This is a **conscious design decision** made for clarity and modularity.

> ğŸ’¡ Rather than mixing activation logic with hidden state or class-level storage, we favor **explicit data flow**: forward results must be saved and passed to backward. Itâ€™s simple, transparent, and avoids surprises.

---

### ğŸ“š Comparison to NNFS (Neural Networks from Scratch)

The book **NNFS** uses class-based activations that **store internal state** (like `self.output`) across the forward and backward phases:

```python
class Activation_Sigmoid:
    def forward(self, inputs):
        self.output = 1 / (1 + np.exp(-inputs))

    def backward(self, dvalues):
        self.dinputs = dvalues * self.output * (1 - self.output)
```

While that design supports clean encapsulation, it can obscure intermediate computations when debugging or learning.

In contrast, this project uses:
- Stateless `@staticmethod` functions  
- Clear data ownership â€” the caller is responsible for saving intermediate values  
- No hidden state or side-effects

This is in line with our educational goal:  
âœ… *See exactly whatâ€™s flowing forward and backward â€” every step, every value.*

---

## âœ… Supported Activations

| Activation | Class Name         | Forward Function                  | Derivative (`backward()`)                    |
|------------|--------------------|-----------------------------------|----------------------------------------------|
| **Tanh**   | `TanhActivation`   | `tanh(x)`                         | `1 - tanhÂ²(x)`                                |
| **Sigmoid**| `SigmoidActivation`| `1 / (1 + exp(-x))`               | `Ïƒ(x)(1 - Ïƒ(x))` where `Ïƒ` is sigmoid        |
| **Softmax**| `SoftmaxActivation`| `e^x / Î£ e^x` (vector output)     | âˆ‚ğ‘áµ¢/âˆ‚ğ‘¥â±¼ = Jacobian matrix (see below)        |
| **ReLU**   | `ReLUActivation`   | `max(0, x)`                       | `1 if x > 0 else 0`                           |

---

## âœ¨ Usage Example

```python
from activations.tanh import TanhActivation

z = np.array([...])  # pre-activation

# Forward pass
a = TanhActivation.forward(z)

# Backward pass (manual gradient computation)
dz = da * TanhActivation.backward(z)
```

---

## ğŸ”¬ Manual Derivatives vs. Autograd (Frameworks)

In this scratch implementation, **you must define both `forward()` and `backward()`**, because we're not using automatic differentiation.

### ğŸ§  Example: Sigmoid

```python
def forward(x):
    return 1 / (1 + np.exp(-x))

def backward(x):
    s = forward(x)
    return s * (1 - s)
```

---

## ğŸ¤– What Do Frameworks Do?

Libraries like **TensorFlow** and **PyTorch**:

- Record all operations in a **computational graph**
- Use **automatic differentiation** (autograd)
- Handle chain rule and derivatives internally

### âœ… TensorFlow Example

```python
import tensorflow as tf

x = tf.Variable([[0.0, 1.0]])
with tf.GradientTape() as tape:
    y = tf.math.sigmoid(x)

dy_dx = tape.gradient(y, x)
print(dy_dx)  # no need to define backward()
```

---

## ğŸ” Summary: Manual vs. Automatic Differentiation

| Feature                 | Scratch RNNs        | TensorFlow / PyTorch    |
|------------------------|---------------------|--------------------------|
| Activation `forward`   | âœ… Required          | âœ… Required               |
| Activation `backward`  | âœ… Required          | âŒ Handled by autograd    |
| Backpropagation Logic  | Manual chain rule    | Automatic gradient engine|
| Flexibility            | ğŸ”§ Fully Customizable| âš¡ Highly Optimized       |

---
## ğŸ“š Reference & Credit

The design of this activation module â€” with explicit `forward()` and `backward()` methods â€” was inspired by:

### ğŸ§  Neural Networks from Scratch in Python  
by *Harrison Kinsley (Sentdex)* and *Daniel KukieÅ‚a*

This book (and YouTube series) walks through neural networks at a truly foundational level, including:

- Manual implementation of activations (sigmoid, tanh, ReLU, etc.)
- Derivative math for backpropagation
- Forward + backward interface design patterns

If you're learning how to build neural nets from first principles, itâ€™s an incredible resource.

- ğŸ”— [Official Site](https://nnfs.io)  
- ğŸ“º [YouTube Playlist](https://www.youtube.com/playlist?list=PLQVvvaa0QuDfSfqQuee6K8opKtZsh7sA9)  
- ğŸ’» [GitHub Code](https://github.com/Sentdex/nnfs)

---

### ğŸ“ Coursera: Machine Learning Specialization

From *Andrew Ng's DeepLearning.AI* â€” this course introduces the theory and math behind many activation functions used in modern neural networks:

- Sigmoid, Tanh, and ReLU explained with derivatives  
- Chain rule applications in backpropagation  
- When to use each function (and why)

- ğŸ§® [Course: Advanced Learning Algorithms](https://www.coursera.org/learn/advanced-learning-algorithms)
- ğŸŒ [Coursera Specialization](https://www.coursera.org/specializations/machine-learning-introduction)

