# RNN-LSTM-from-Scratch

Welcome to **RNN-LSTM-from-Scratch** â€” a hands-on lab where we **build deep learning models from the ground up**, using only **NumPy**.

Learn how **recurrent neural networks** (RNNs) and **long short-term memory networks** (LSTMs) really work â€” no black boxes, no shortcuts.

---

## ğŸš€ Why This Project?
- Master the **nuts and bolts** of RNNs and LSTMs by building everything yourself.
- See exactly **how models learn** through **forward passes**, **backward passes**, and **optimizer updates**.
- Compare **pure NumPy training** vs **TensorFlow workflows** â€” side-by-side.
- Build, train, debug, and **generate text** with models you fully understand.

---

## ğŸ§  What's Inside

### âœ… Core Features
- `CharTokenizer`: Maps characters â†”ï¸ indices (with OOV support)
- `data_prep.py`: Load text data and create training sequences
- `rnn_model.py`: RNN core logic (forward, backward, sampling, gradient clipping)

- **Training Scripts**:
  - `scratch_char_level_rnn_model.py`: Single example training (NumPy)
  - `scratch_char_level_rnn_batch_train.py`: Mini-batch training (NumPy)
  - `tf_char_level_rnn_model.py`: TensorFlow model (`model.fit` API)
  - `tf_char_rnn_manual_train.py`: TensorFlow model with manual training loop
  - `tf_char_rnn.py`: TensorFlow model class (`TFCharRNN`)

- `utils.py`: Helper functions: softmax, loss smoothing, random seeds
- `optimizers.py`: Custom-built SGD, RMSProp, and Adam optimizers

âœ… See detailed optimizer docs [**here**](README_OPTIMIZER.md).

---

## ğŸ¤– Two Ways to Train
| Mode                 | Framework        | Description |
|:---------------------|:-----------------|:------------|
| **From Scratch**      | NumPy             | Full manual RNN training with explicit forward/backward passes |
| **TensorFlow**        | TensorFlow (Keras) | High-level training for benchmarking and comparison |

---

âœ… TensorFlow versions (in `tf_char_*` files) are **fully isolated** and **not used** in scratch training.  
âœ… Compare the low-level and high-level approaches **side-by-side**.

---

## ğŸ“ˆ Features Overview

| Feature                   | NumPy (Scratch) | TensorFlow Version | Status |
|:---------------------------|:----------------|:-------------------|:------:|
| Character tokenizer         | âœ… | âœ… | Complete |
| Manual forward pass         | âœ… | âœ… | Complete |
| Manual backward pass (BPTT) | âœ… | âœ… | Complete |
| Mini-batching               | âœ… | âœ… | Complete |
| Clipping gradients          | âœ… | âœ… | Complete |
| Training with `model.fit`   | âŒ | âœ… | Complete |
| Manual training loop        | âœ… | âœ… | Complete |
| Sampling (temperature)      | âœ… | âœ… | Complete |
| LSTM cell                   | ğŸ”œ | ğŸ”œ | Coming Soon |
| Optimizers (SGD, RMSProp, Adam) | âœ… | âœ… | Complete |

---

## âœ… LSTM Tests

This repo includes a full from-scratch LSTM implementation with detailed unit tests (forward, backward, gradient check, Keras comparison).

ğŸ“„ See [`tests/lstm/README.md`](tests/lstm/README.md) for full test breakdown.

---

## ğŸ“‚ Project Structure
```
rnn-lstm-from-scratch/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/                   # Images for visualizations
â”‚   â”œâ”€â”€ dinos.txt                  # Dinosaur name corpus
â”‚   â””â”€â”€ shakespeare.txt            # Shakespeare plays corpus
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ optimizers/                # Custom optimizers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ adam_optimizer.py
â”‚   â”‚   â”œâ”€â”€ momentum_optimizer.py
â”‚   â”‚   â”œâ”€â”€ optimizer.py           # Base optimizer class
â”‚   â”‚   â”œâ”€â”€ rmsprop_optimizer.py
â”‚   â”‚   â””â”€â”€ sgd_optimizer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ char_level_rnn_model.py    # Character-level RNN (NumPy)
â”‚   â”œâ”€â”€ data_prep.py               # Dataset loading and preparation
â”‚   â”œâ”€â”€ rnn_model.py               # Scratch RNN core (forward, backward)
â”‚   â”œâ”€â”€ scratch_char_level_rnn_model_batch.py  # Scratch trainer (mini-batch)
â”‚   â”œâ”€â”€ scratch_char_level_rnn_model.py        # Scratch trainer (single example)
â”‚   â”œâ”€â”€ text_dataset.py            # Text dataset utilities
â”‚   â”œâ”€â”€ tf_char_level_rnn_model.py # TensorFlow model with .fit
â”‚   â”œâ”€â”€ tf_char_rnn_manual_train.py # TensorFlow manual training loop
â”‚   â”œâ”€â”€ tf_char_rnn.py             # TensorFlow RNN model class
â”‚   â”œâ”€â”€ tokenizer.py               # CharTokenizer
â”‚   â””â”€â”€ utils.py                   # Helper functions (softmax, loss smoothing, etc.)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ optimizers/
â”‚   â”‚   â”œâ”€â”€ test_adam_optimizer.py
â”‚   â”‚   â”œâ”€â”€ test_momentum_optimizer.py
â”‚   â”‚   â”œâ”€â”€ test_rmsprop_optimizer.py
â”‚   â”‚   â””â”€â”€ test_sgd_optimizer.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ README_OPTIMIZER.md            # Optimizer-specific documentation
â”œâ”€â”€ requirements.txt
```

---

## ğŸ“š Datasets
- `dinos.txt` â€” Dinosaur names
- `shakespeare.txt` â€” Shakespeare plays

*Want to train on your own text?*  
Just drop a `.txt` file into the `data/` folder â€” youâ€™re ready to go!

---

## âœï¸ Example Output
After training on dinosaur names:
```
Generated: "Brontosaurus"
Generated: "Stegoceratops"
Generated: "Trodonax"
```

---

## ğŸ› ï¸ Coming Soon
| Feature | Status |
|:--------|:-------|
| LSTM Cell from scratch | ğŸ”œ In Progress |
| GRU Cell from scratch | ğŸ”œ In Progress |
| Model checkpointing | â³ |
| Attention mechanism exploration | ğŸ§  Future |

---

## ğŸ§© Built With
- Python 3.10+
- NumPy (pure scratch mode)
- TensorFlow (comparison mode)

---

## ğŸ“¬ Contributing
Pull Requests welcome!  
This is a **learning-first lab** for anyone who wants to truly understand RNNs and LSTMs â€” no magic, no shortcuts. ğŸ§ª

---

## ğŸ“ Learn More

To deepen your understanding of the building blocks behind RNNs and LSTMs:

- ğŸ“˜ [Neural Networks from Scratch (Book)](https://nnfs.io/) â€“ Great resource for understanding manual backprop, activations, and optimizers from first principles.
- ğŸ§  [Coursera NLP Sequence Models](https://www.coursera.org/learn/nlp-sequence-models/home/week/1) â€“ Excellent series covering RNNs, GRUs, LSTMs, and BPTT.
- ğŸ”„ [Backpropagation Through Time (BPTT)](https://www.coursera.org/learn/nlp-sequence-models/lecture/bc7ED/backpropagation-through-time) â€“ Understand how gradients flow across time steps.
- ğŸ¯ [Hyperparameter Tuning & Optimization](https://www.coursera.org/learn/deep-neural-network) â€“ For training stability and performance improvements.

> Want to understand how `tanh`, `sigmoid`, and `softmax` really work?  
> ğŸ“‚ Check out [`README_ACTIVATION.md`](./README_ACTIVATION.md) for hand-coded formulas, derivatives, and examples â€” no magic, just math.

---

Happy building ğŸ”  

Let's make it learn ğŸ¦– â¡ï¸ ğŸ“  
ğŸ¦– Train it. ğŸ” Backprop it. âœï¸ Sample it.  
Build deep, learn deeper.

Become one with the gradients.
