# RNN-LSTM-from-Scratch

Welcome to **RNN-LSTM-from-Scratch** â€” a hands-on lab where we **build deep learning models from the ground up**, using only **NumPy**.

Learn how **recurrent neural networks** (RNNs) and **long short-term memory networks** (LSTMs) really work â€” no black boxes, no shortcuts.

---

## ğŸ“š Table of Contents

- [RNN-LSTM-from-Scratch](#rnn-lstm-from-scratch)
  - [ğŸ“š Table of Contents](#-table-of-contents)
  - [ğŸš€ Why This Project?](#-why-this-project)
  - [ğŸ¯ Project Philosophy](#-project-philosophy)
  - [ğŸ§  What's Inside](#-whats-inside)
    - [âœ… Core Components (NumPy)](#-core-components-numpy)
    - [ğŸ§  Models (`src/models/`)](#-models-srcmodels)
    - [ğŸ§® Activations (`src/activations/`)](#-activations-srcactivations)
    - [ğŸ‹ï¸ Training Scripts](#ï¸-training-scripts)
    - [âš™ï¸ Optimizers (`src/optimizers/`)](#ï¸-optimizers-srcoptimizers)
  - [ğŸ¤– Two Ways to Train](#-two-ways-to-train)
  - [ğŸ“ˆ Features Overview](#-features-overview)
  - [ğŸ§  Design Decisions](#-design-decisions)
    - [â—Logits instead of Softmax in the Forward Pass](#logits-instead-of-softmax-in-the-forward-pass)
    - [ğŸ” Clean Gradient Flow (Forward â†’ Loss â†’ Backward)](#-clean-gradient-flow-forward--loss--backward)
  - [âœ… Tests](#-tests)
    - [ğŸ” RNN Tests](#-rnn-tests)
    - [ğŸ§  LSTM Tests](#-lstm-tests)
  - [ğŸ“ Project Layout](#-project-layout)
  - [ğŸ“š Datasets](#-datasets)
  - [âš™ï¸ Setup](#ï¸-setup)
  - [ğŸ§ª Running the Code](#-running-the-code)
    - [ğŸ”¹ Single-Example Training](#-single-example-training)
      - [CLI Arguments](#cli-arguments)
    - [ğŸ”¹ Mini-Batch Training](#-mini-batch-training)
      - [CLI Arguments](#cli-arguments-1)
    - [ğŸ”¹ LSTM Training (From Scratch)](#-lstm-training-from-scratch)
      - [CLI Arguments](#cli-arguments-2)
  - [âš¡ Quickstart](#-quickstart)
    - [ğŸ” Tip: Reset the environment if needed](#-tip-reset-the-environment-if-needed)
  - [âœï¸ Example Output](#ï¸-example-output)
  - [ğŸ› ï¸ Coming Soon](#ï¸-coming-soon)
  - [ğŸ§© Built With](#-built-with)
  - [ğŸ“¬ Contributing](#-contributing)
  - [ğŸ“ Learn More](#-learn-more)

---

## ğŸš€ Why This Project?
- Master the **nuts and bolts** of RNNs and LSTMs by building everything yourself.
- See exactly **how models learn** through **forward passes**, **backward passes**, and **optimizer updates**.
- Compare **pure NumPy training** vs **TensorFlow workflows** â€” side-by-side.
- Build, train, debug, and **generate text** with models you fully understand.

---

## ğŸ¯ Project Philosophy

This project isn't just about repeating what's taught â€” it's about breaking things down, rebuilding them, and truly understanding **how neural networks tick**. We dig under the hood to understand the math, the matrix ops, and the mechanics that make RNNs and LSTMs work.

Our goal is to **learn by building**, not just by using. That means stepping away from the Keras high-level API and rebuilding each part from scratch â€” even if itâ€™s harder, slower, and less elegant. The result? You walk away understanding not just *what* to do, but *why* it works.

> Think of this as a laboratory for neural networks â€” part engineering, part science fair, part curiosity project.

---

## ğŸ§  What's Inside

### âœ… Core Components (NumPy)
- `CharTokenizer`: Maps characters â†”ï¸ indices (with OOV support)
- `data_prep.py`: Loads text and prepares training sequences
- `utils.py`: Utility functions for `softmax`, loss smoothing, padding, clipping
- `sampling.py`: Shared sampling logic (RNN, LSTM) with temperature scaling
- `compute_loss_and_grad`: Combines cross-entropy loss with softmax + âˆ‚L/âˆ‚logits, mirroring modern deep learning loss handling (`from_logits=True`).
- `grad_utils.py`: Functions to project logits â†’ hidden gradients and compute output layer (`dWy`, `dby`) gradients

---

### ğŸ§  Models (`src/models/`)
- `rnn_model.py`: Recurrent Neural Network (RNN)
  - Forward pass, backpropagation through time (BPTT), loss via logits
- `lstm_model.py`: Long Short-Term Memory (LSTM)
  - Forward pass, backward pass, modular activation support

---

### ğŸ§® Activations (`src/activations/`)
- `tanh.py`: Hyperbolic tangent with manual forward/backward
- `sigmoid.py`: Sigmoid activation with manual gradients
- `softmax.py`: Temperature-scaled softmax (vectorized + single-column)
- `base.py`: Abstract activation interface (for modular design)

ğŸ“„ See [`README_ACTIVATION.md`](README_ACTIVATION.md) for full math, gradients, and usage notes.

---

### ğŸ‹ï¸ Training Scripts
- `scratch_char_level_rnn_model.py`: Single-example RNN training (manual)
- `scratch_char_level_lstm_model.py`: Single-example LSTM training (manual)
- `scratch_char_level_rnn_model_batch.py`: Mini-batch RNN training (manual)
- `tf_char_level_rnn_model.py`: TensorFlow model (`model.fit`)
- `tf_char_rnn_manual_train.py`: TensorFlow with manual training loop
- `tf_char_rnn.py`: TensorFlow RNN class (`TFCharRNN`)

---

### âš™ï¸ Optimizers (`src/optimizers/`)
- `SGDOptimizer`: Basic stochastic gradient descent
- `MomentumOptimizer`: SGD with momentum
- `RMSPropOptimizer`: Adaptive learning with RMS decay
- `AdamOptimizer`: Adaptive Moment Estimation (Adam)

ğŸ“„ Full docs: [`README_OPTIMIZER.md`](README_OPTIMIZER.md)

---

## ğŸ¤– Two Ways to Train
| Mode                 | Framework        | Description |
|:---------------------|:-----------------|:------------|
| **From Scratch**      | NumPy             | Full manual RNN training with explicit forward/backward passes |
| **TensorFlow**        | TensorFlow (Keras) | High-level training for benchmarking and comparison |

---

âœ… TensorFlow versions (in `tf_char_*` files) are **fully isolated** and **not used** in scratch training.  
âœ… Compare the low-level and high-level approaches **side-by-side**.

---

## ğŸ“ˆ Features Overview

| Feature                   | NumPy (Scratch) | TensorFlow Version | Status |
|:---------------------------|:----------------|:-------------------|:------:|
| Character tokenizer         | âœ… | âœ… | Complete |
| Manual forward pass         | âœ… | âœ… | Complete |
| Manual backward pass (BPTT) | âœ… | âœ… | Complete |
| Mini-batching               | âœ… | âœ… | Complete |
| Clipping gradients          | âœ… | âœ… | Complete |
| Training with `model.fit`   | âŒ | âœ… | Complete |
| Manual training loop        | âœ… | âœ… | Complete |
| Sampling (temperature)      | âœ… | âœ… | Complete |
| LSTM cell                   | âœ…  | âŒ | Complete |
| Optimizers (SGD, RMSProp, Adam) | âœ… | âœ… | Complete |

---
Absolutely â€” here's the updated full **ğŸ§  Design Decisions** section including your new gradient flow philosophy:

---

## ğŸ§  Design Decisions

This project **intentionally deviates** from the Coursera implementation in key places to align better with deep learning best practices:

---

### â—Logits instead of Softmax in the Forward Pass

Unlike the original course, we do **not apply softmax during the forward pass**. Instead, we return the raw output logits and defer softmax to the loss function or sampling step.

**Why?**

- âœ… This mirrors modern deep learning frameworks like TensorFlow and PyTorch, where loss functions (e.g. `categorical_crossentropy`) handle softmax internally via `from_logits=True`.
- âœ… It improves **numerical stability** and reduces unnecessary computation during training.
- âœ… It lets us compute a **vectorized softmax** once during loss calculation, instead of step-by-step during the forward pass.

This was a **conscious architectural decision** â€” not a shortcut.  
It helps us better debug gradients, align with frameworks, and prepare for deeper experiments like temperature sampling and attention mechanisms.

---

### ğŸ” Clean Gradient Flow (Forward â†’ Loss â†’ Backward)

We restructured the training pipeline to follow a more **modular and intuitive gradient flow**:

> `Forward â†’ Loss (+ dy) â†’ da â†’ Backward â†’ Output Layer Gradients â†’ Update`

This change was driven by confusion around the original implementation, which computed the loss *inside* the backward function.  
After going through previous ML and deep learning course material, this design felt inconsistent â€” **why would the loss be calculated during backprop?** It broke the expected flow and made reasoning about gradients harder than it needed to be to me.

**Why this design is cleaner and easier to understand:**

- âœ… Each step in the training loop does one thing: forward pass, loss computation, gradient propagation, parameter updates.
- âœ… Easier to debug, test, and visualize: `dy` (âˆ‚L/âˆ‚z) and `da` (âˆ‚L/âˆ‚a) are explicit, inspectable intermediates.
- âœ… Aligns with best practices in frameworks like TensorFlow and PyTorch, where `loss.backward()` happens outside model logic.
- âœ… Sets the stage for adding flexible features like different loss functions, regularization, or advanced optimizers.

This cleanup significantly improved both the structure of the code and the *clarity of the learning process*.

---

## âœ… Tests

### ğŸ” RNN Tests

The RNN implementation is backed by a comprehensive test suite to ensure forward and backward logic are both mathematically correct and consistent with TensorFlow.

- âœ… Validates `rnn_cell_step()`, `rnn_forward()`, and `rnn_backward()` end-to-end
- ğŸ”¬ Compares gradients with Keras using `GradientTape`
- ğŸ“‰ Confirms loss decreases over synthetic training loops
- ğŸ§ª Tests sampling output: sequence length, vocab conformity, temperature effects

ğŸ“„ See [`tests/rnn/README.md`](tests/rnn/README.md) for a full breakdown.  
All tests are deterministic, self-contained, and require **no external data**.

---

### ğŸ§  LSTM Tests

This repo includes a from-scratch LSTM implementation with detailed unit tests for:

- Forward/backward cell logic
- Gradient shape and value correctness
- Cross-validation against Keras `LSTMCell`

ğŸ“„ Full breakdown in [`tests/lstm/README.md`](tests/lstm/README.md)

---

## ğŸ“ Project Layout

- `src/` â€” Core training and model code (NumPy + TensorFlow)
  - `models/` â€” From-scratch model logic
    - `rnn_model.py`, `lstm_model.py`
  - `data_prep.py`, `tokenizer.py`, `utils.py`, `sampling.py`
  - `scratch_*.py` (from-scratch trainers)
  - `tf_*.py` (TensorFlow trainers)
  - `activations/` â€” Hand-coded activation functions ğŸ”¬
  - `optimizers/` â€” Custom SGD, Momentum, RMSProp, Adam ğŸ’¡

- `tests/` â€” Unit tests for RNN, LSTM, optimizers, and sampling
- `data/` â€” Datasets like `dinos.txt`, `shakespeare.txt`
- `README_ACTIVATION.md`, `README_OPTIMIZER.md` â€” Docs for custom components

---

## ğŸ“š Datasets
- `dinos.txt` â€” Dinosaur names
- `shakespeare.txt` â€” Shakespeare plays

*Want to train on your own text?*  
Just drop a `.txt` file into the `data/` folder â€” youâ€™re ready to go!

---


## âš™ï¸ Setup

Clone the repo and install dependencies:

```bash
git clone https://github.com/kjpou1/rnn-lstm-from-scratch.git
cd rnn-lstm-from-scratch
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
```

Set the Python path to allow module-based execution:

```bash
export PYTHONPATH=.
```

Or add it permanently to your shell profile (`.zshrc`, `.bashrc`, etc.):

```bash
echo 'export PYTHONPATH=.' >> ~/.zshrc
```

---



## ğŸ§ª Running the Code

This project includes two scratch-built NumPy training scripts:

- ğŸ§¬ `scratch_char_level_rnn_model.py`: single-example RNN training
- ğŸ§ª `scratch_char_level_rnn_model_batch.py`: mini-batch RNN training

> âœ… Before running either script, be sure to set:
```bash
export PYTHONPATH=.
```

---

### ğŸ”¹ Single-Example Training

```bash
python -m src.scratch_char_level_rnn_model

or

PYTHONPATH=. python -m src.scratch_char_level_rnn_model
```

#### CLI Arguments

| Argument            | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| `--dataset`         | Dataset name (e.g. `dinos` â†’ `data/dinos.txt`)                              |
| `--iterations`      | Number of training iterations                                               |
| `--learning_rate`   | Learning rate for gradient descent                                          |
| `--optimizer`       | Optimizer type: `sgd`, `momentum`, `rms`, or `adam`                         |
| `--temperature`     | Sampling temperature: <br>`<1` = more deterministic, `>1` = more creative   |
| `--hidden_size`     | Number of RNN hidden units                                                  |
| `--sample_every`    | Print sampled text every N iterations                                       |
| `--seq_length`      | Maximum length of generated samples                                         |
| `--clip_value`      | Gradient clipping threshold                                                 |

---

### ğŸ”¹ Mini-Batch Training

```bash
python -m src.scratch_char_level_rnn_model_batch

or 

PYTHONPATH=. python -m src.scratch_char_level_rnn_model_batch
```

#### CLI Arguments

| Argument            | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| `--dataset`         | Dataset name (e.g. `dinos`)                                                 |
| `--epochs`          | Number of training epochs (full data passes)                                |
| `--batch_size`      | Size of mini-batches                                                        |
| `--learning_rate`   | Learning rate                                                               |
| `--optimizer`       | Optimizer type: `sgd`, `momentum`, `rms`, or `adam`                         |
| `--temperature`     | Sampling temperature                                                        |
| `--hidden_size`     | Hidden layer size                                                           |
| `--seq_length`      | Length of generated sequences                                               |
| `--clip_value`      | Max allowed gradient norm (clipping)                                       |
| `--deterministic`   | Set this flag for deterministic shuffling (reproducibility)                |

> The mini-batch script uses **line-by-line training** and applies `pad_sequences()` to handle variable input lengths.

---

### ğŸ”¹ LSTM Training (From Scratch)

```bash
python -m src.scratch_char_level_lstm_model

or

PYTHONPATH=. python -m src.scratch_char_level_lstm_model
```

#### CLI Arguments

| Argument            | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| `--dataset`         | Dataset name (e.g. `dinos` â†’ `data/dinos.txt`)                              |
| `--iterations`      | Number of training iterations                                               |
| `--learning_rate`   | Learning rate for gradient descent                                          |
| `--optimizer`       | Optimizer type: `sgd`, `momentum`, `rms`, or `adam`                         |
| `--temperature`     | Sampling temperature: <br>`<1` = more deterministic, `>1` = more creative   |
| `--hidden_size`     | Number of LSTM hidden units                                                 |
| `--sample_every`    | Print sampled text every N iterations                                       |
| `--seq_length`      | Maximum length of generated samples                                         |
| `--clip_value`      | Gradient clipping threshold                                                 |

---

âœ… This training script follows the **clean gradient flow** philosophy:

> `Forward â†’ Loss (+ dy) â†’ da â†’ Backward â†’ Output Layer Gradients â†’ Update`

No loss is computed inside the backward pass. You explicitly compute:

- `dy = âˆ‚L/âˆ‚z` (via `compute_loss_and_grad`)
- `da = âˆ‚L/âˆ‚a` (via `project_logit_grad_to_hidden`)
- Then pass `da` into `lstm_backwards`

This keeps the LSTM model logic modular and easy to test.

---

Here's a polished **Quickstart** section you can drop directly into your `README.md`. Iâ€™ll also tell you where to place it.

---


## âš¡ Quickstart

Want to skip the flags and just run something?

Here are 3 quick commands to get you training instantly â€” one for each mode:

```bash
# ğŸ§ª Mini-batch RNN training (NumPy)
python -m src.scratch_char_level_rnn_model_batch --dataset dinos --epochs 20 --optimizer adam --learning_rate 0.005

# ğŸ§¬ Single-example RNN training (NumPy)
python -m src.scratch_char_level_rnn_model --dataset dinos --iterations 22001 --sample_every 2000 --optimizer rms --learning_rate 0.005

# ğŸ§  Single-example LSTM training (NumPy)
python -m src.scratch_char_level_lstm_model --dataset dinos --iterations 22001 --sample_every 2000 --optimizer adam --learning_rate 0.005
```

No config files, no magic â€” just raw NumPy and CLI args.  
Each script prints sample output every few steps so you can **see it learn live.**

---

### ğŸ” Tip: Reset the environment if needed

```bash
export PYTHONPATH=.
```

---


## âœï¸ Example Output
After training on dinosaur names:
```
--- Generating samples:
Tonisaurus

Opandeniaurus

Onosaurus

Tonisaurus

Yptops

Eunthus

Yosaurus
```

---

## ğŸ› ï¸ Coming Soon
| Feature | Status |
|:--------|:-------|
| GRU Cell from scratch | ğŸ”œ In Progress |
| Model checkpointing | â³ |
| Attention mechanism exploration | ğŸ§  Future |

---

## ğŸ§© Built With
- Python 3.10+
- NumPy (pure scratch mode)
- TensorFlow (comparison mode)

---

## ğŸ“¬ Contributing
Pull Requests welcome!  
This is a **learning-first lab** for anyone who wants to truly understand RNNs and LSTMs â€” no magic, no shortcuts. ğŸ§ª

---

## ğŸ“ Learn More

To deepen your understanding of the building blocks behind RNNs and LSTMs:

- ğŸ“˜ [Neural Networks from Scratch (Book)](https://nnfs.io/) â€“ Great resource for understanding manual backprop, activations, and optimizers from first principles.
- ğŸ§  [Coursera NLP Sequence Models](https://www.coursera.org/learn/nlp-sequence-models/home/week/1) â€“ Excellent series covering RNNs, GRUs, LSTMs, and BPTT.
- ğŸ”„ [Backpropagation Through Time (BPTT)](https://www.coursera.org/learn/nlp-sequence-models/lecture/bc7ED/backpropagation-through-time) â€“ Understand how gradients flow across time steps.
- ğŸ¯ [Hyperparameter Tuning & Optimization](https://www.coursera.org/learn/deep-neural-network) â€“ For training stability and performance improvements.

> Want to understand how `tanh`, `sigmoid`, and `softmax` really work?  
> ğŸ“‚ Check out [`README_ACTIVATION.md`](./README_ACTIVATION.md) for hand-coded formulas, derivatives, and examples â€” no magic, just math.

---

Happy building ğŸ”  

Let's make it learn ğŸ¦– â¡ï¸ ğŸ“  
ğŸ¦– Train it. ğŸ” Backprop it. âœï¸ Sample it.  
Build deep, learn deeper.

Become one with the gradients.
