# RNN-LSTM-from-Scratch

This project implements character-level **RNNs and LSTMs** **from scratch** using only **NumPy**, alongside **TensorFlow versions** for comparison.

Itâ€™s a hands-on deep dive into **how RNNs and LSTMs really work â€” from first principles.**

---

## ğŸš€ Goals
- Understand and **implement RNNs and LSTMs** from scratch
- Train models to generate text character-by-character
- Explore **forward and backward passes** (including BPTT)
- Apply **gradient clipping** to stabilize training
- Build **custom optimizers** (SGD, RMSProp, Adam)
- Compare **scratch NumPy** vs **TensorFlow** workflows side-by-side

---

## ğŸ§  What's Inside

### âœ… Core Features
- `CharTokenizer`: Maps characters â†”ï¸ indices (with OOV support)
- `data_prep.py`: Load raw text and create training sequences
- `rnn_model.py`: RNN core logic (forward, backward, sampling, clipping)

- **Training Scripts**:
  - `scratch_char_level_rnn_model.py`: Single example iteration training (NumPy)
  - `scratch_char_level_rnn_batch_train.py`: Mini-batch training (NumPy)
  - `tf_char_level_rnn_model.py`: TensorFlow model (uses `model.fit`)
  - `tf_char_rnn_manual_train.py`: TensorFlow **manual training loop** (custom batches)
  - `tf_char_rnn.py`: TensorFlow model class (`TFCharRNN`)

- `utils.py`: Helper functions: softmax, loss smoothing, random seeds, etc.
- `optimizers.py`: Custom optimizer implementations (SGD, RMSProp, Adam)

---

## ğŸ¤– Two Modes of Training
| Mode                 | Framework        | Description |
|:---------------------|:-----------------|:------------|
| **Scratch RNN**       | NumPy             | Full manual implementation (educational focus) |
| **TensorFlow RNN**    | TensorFlow (Keras) | Used for comparison (manual batch training + fit training) |

---

âœ… TensorFlow versions (in `tf_char_*` files) are **fully isolated** and **not used** in scratch RNN training.

âœ… This lets you **compare side-by-side**:
- *From-scratch NumPy workflow* vs *TensorFlow/Keras best practices*  
- *Manual gradient updates* vs *automatic optimizers*

---

## ğŸ“ˆ Features Table

| Feature            | NumPy (Scratch) | TensorFlow Version | Status |
|:-------------------|:----------------|:-------------------|:------:|
| Character tokenizer | âœ… | âœ… | Complete |
| Manual forward pass | âœ… | âœ… | Complete |
| Manual backward pass (BPTT) | âœ… | âœ… | Complete |
| Mini-batching        | âœ… | âœ… | Complete |
| Clipping gradients   | âœ… | âœ… | Complete |
| Training with model.fit | âŒ | âœ… | Complete |
| Manual training loop | âœ… | âœ… | Complete |
| Sampling (temperature) | âœ… | âœ… | Complete |
| LSTM cell            | ğŸ”œ | ğŸ”œ | Coming Soon |
| Optimizers (SGD, RMSProp, Adam) | âœ… | âœ… | Complete |

---

## ğŸ“‚ Project Structure
```
rnn-lstm-from-scratch/
â”œâ”€â”€ data/                         # Text corpora (e.g., dinos, Shakespeare)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tokenizer.py              # CharTokenizer
â”‚   â”œâ”€â”€ data_prep.py              # Dataset loading and preparation
â”‚   â”œâ”€â”€ rnn_model.py              # Scratch RNN core (forward, backward)
â”‚   â”œâ”€â”€ scratch_char_level_rnn_model.py  # Iteration-based scratch trainer
â”‚   â”œâ”€â”€ scratch_char_level_rnn_batch_train.py  # Batch-based scratch trainer
â”‚   â”œâ”€â”€ tf_char_rnn.py            # TensorFlow RNN Model Class
â”‚   â”œâ”€â”€ tf_char_level_rnn_model.py # TensorFlow model with .fit
â”‚   â”œâ”€â”€ tf_char_rnn_manual_train.py # TensorFlow model with manual training
â”‚   â”œâ”€â”€ utils.py                  # Softmax, smoothing, random seed, etc.
â”‚   â”œâ”€â”€ optimizers.py             # Custom optimizer implementations
â””â”€â”€ README.md
```

---

## ğŸ“š Datasets
- `dinos.txt` â€” Dinosaur names
- `shakespeare.txt` â€” Shakespeare plays

You can add your own character-level corpus easily (just drop a `.txt` file into `data/`).

---

## âœï¸ Example Output
After training on dinosaur names:
```
Generated: "Brontosaurus"
Generated: "Stegoceratops"
Generated: "Trodonax"
```
---

## ğŸ› ï¸ Coming Soon
| Feature | Status |
|:--------|:-------|
| LSTM Cell from scratch | ğŸ”œ In Progress |
| GRU Cell from scratch | ğŸ”œ In Progress |
| Model checkpointing | â³ |
| Attention mechanism exploration | ğŸ§  Future |

---

## ğŸ§© Built With
- Python 3.10+
- NumPy only (pure scratch)
- No TensorFlow or PyTorch for scratch models

---

## ğŸ“¬ Contributing
Pull Requests welcome!  
We are building a true **from-scratch RNN-LSTM lab** ğŸ§ª

---

## ğŸ§  Learn More
- [Coursera NLP Sequence Models](https://www.coursera.org/learn/nlp-sequence-models/home/week/1)
- [Backpropagation Through Time (BPTT)](https://www.coursera.org/learn/nlp-sequence-models/lecture/bc7ED/backpropagation-through-time)
- [Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network)

---

Happy building ğŸ”  
Let's make it learn ğŸ¦– â¡ï¸ ğŸ“
